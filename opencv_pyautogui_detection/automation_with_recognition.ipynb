{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0694a3d6-7d51-4ee4-89f2-6be6b28b0b5d",
   "metadata": {},
   "source": [
    "# Automated Game Playing With OpenCV and PyAutoGUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e6f25-cb87-437f-8209-ef6fd3220e70",
   "metadata": {},
   "source": [
    "We have already seen how we can automate everyday activities with PyAutoGUI. We will now see how we can use OpenCV image recognition with PyAutoGUI. We will use image recognition with OpenCV to send signals to which we will interpret as actions to our game and we will play them using PyAutoGUI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3be1c-6268-409f-9d88-321386abe2fe",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751ddd7-e542-4b24-a4ca-273b946ab0be",
   "metadata": {},
   "source": [
    "Our workflow involves two steps:\n",
    "* Face recognition with OpenCV\n",
    "* Automation with PyAutoGUI\n",
    "\n",
    "**Image recognition**\n",
    "<br>\n",
    "Here we will have the following steps:\n",
    "* Load a model\n",
    "* Capture frames from webcam\n",
    "* Feed the model with frames\n",
    "* Get predictions from the model\n",
    "* Detect whether face has left centre bounding box\n",
    "* If it has left, make the appropriate move\n",
    "\n",
    "**Automation**\n",
    "<br>\n",
    "Once we detect a move outside the bounding box, we call PyAutoGui for automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ea806-a847-4082-a6ac-61bc8576d4da",
   "metadata": {},
   "source": [
    "OpenCV supports deep neural networks through its `dnn` module. It has methods to load models from various frameworks such as TensorFlow, PyTorch and Caffe. We can also use it to get predictions on input into a model.\n",
    "<br>\n",
    "For this example, we are going to use a Caffe framework model for face detection. We will use OPenCV's `readnetfromCaffe` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b13bf0-87a7-4eff-b667-cacc4274017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0a54e-5d6d-4b98-bffc-43a7d1040d7a",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0722c48-d48e-49f7-8207-b772b339cb35",
   "metadata": {},
   "source": [
    "We will begin by describing all the methods that we will use and then go on to the final flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b2c76-0352-446a-9b54-0ad6c7dc5e48",
   "metadata": {},
   "source": [
    "### Draw Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd412600-e5c0-44de-b66c-71d0b3669581",
   "metadata": {},
   "source": [
    "We define a simple function to draw a rectangle on the screen. This function will be used to draw the bounding box, and face position on a frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364e53b3-f72c-43c4-8c97-b0e0e28262a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(top, bottom, frame, color=(0, 255, 0)):\n",
    "    '''\n",
    "    Given coordinates and a frame, draw a rectangle.\n",
    "\n",
    "    Takes two tuples, top and bottom and the frame,\n",
    "    Optionally takes a color\n",
    "    Returns a copy of the frame, redrawn.\n",
    "    '''\n",
    "\n",
    "    cv2.rectangle(frame, top, bottom, color=color, thickness=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc676d61-2dfb-4f3b-aa87-65fe3a31c515",
   "metadata": {},
   "source": [
    "### Press Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33bd6dc-29f0-4327-990f-967e952f6b1c",
   "metadata": {},
   "source": [
    "This function simply takes as input the hexcode of a key, and presses it. For this, we will use the win32api instead of pyautogui. This is because, for a game, it becomes a bit too slow to use pyautogui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f8e8c3-03a4-4940-9124-f6397534245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32api\n",
    "import win32con\n",
    "\n",
    "def press_key(hexKeyCode):\n",
    "    win32api.keybd_event(hexKeyCode, 0, 0, 0)  # Press key\n",
    "    # Release key\n",
    "    win32api.keybd_event(hexKeyCode, 0,\\\n",
    "                         win32con.KEYEVENTF_KEYUP, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b69672-2944-42b2-b17e-498eaaeff6f0",
   "metadata": {},
   "source": [
    "### Select Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58331c91-c8b4-43e0-8ae9-6b6f7ec7119c",
   "metadata": {},
   "source": [
    "This function works with the `press_key` function above. It selects a key based on user movement and passes it to `press_key` for pressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af61d696-74e9-4af6-9a54-f5e60788ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_key(key:str):\n",
    "    '''\n",
    "    Make a keystroke, depending on the key pressed.\n",
    "\n",
    "    It takes the returns from face_inbox and presses\n",
    "    a set key.\n",
    "\n",
    "    You can customize for a single or for all keys.\n",
    "    '''\n",
    "\n",
    "    if key == 'left':\n",
    "        press_key(win32con.VK_LEFT)\n",
    "    elif key == 'right':\n",
    "        press_key(win32con.VK_RIGHT)\n",
    "    elif key == 'up':\n",
    "        press_key(win32con.VK_UP)\n",
    "    elif key == 'down':\n",
    "        press_key(win32con.VK_DOWN)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87161de6-e40c-4837-82f4-660b51fc32a0",
   "metadata": {},
   "source": [
    "We use this function to press the 4 arrow keys (since they are used in most games). Although, we can customize it to any key(s) we want. The function takes a string which could either be 'left', 'right', 'up' or 'down'. These values show the relative position of the face w.r.t the bounding box of the frame, the centre. We check to see if the face is within the bounding box, if true, it is in the 'centre' in which case no action is taken. If it is outside the bounding box, we return either of the values showing the position and press the necessary key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c5479-d863-45f6-b283-5484bb108f9f",
   "metadata": {},
   "source": [
    "### Get position of face w.r.t bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b8160-9fee-40ca-9d2c-98db15e5b163",
   "metadata": {},
   "source": [
    "We now create a method that will tell us whether or not the face is within the bounding box. The output of this function is the input of the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a605fcb2-0005-4037-a425-3d006f8ca5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_inbox(bbox, face_coords):\n",
    "    '''\n",
    "    This function will check whether the face is\n",
    "    in the bounding box or not.\n",
    "\n",
    "    It takes two 4 value tuples:\n",
    "    (topx, topy, bottx, botty)\n",
    "\n",
    "    Returns 'left', 'right', 'center', 'down' or 'up'\n",
    "    depending on the position of the inner box wrt the\n",
    "    bounding box\n",
    "    '''\n",
    "\n",
    "    if(face_coords[0] < bbox[0]):\n",
    "        return 'left'\n",
    "    elif(face_coords[1] < bbox[1]):\n",
    "        return 'up'\n",
    "    elif(face_coords[2] > bbox[2]):\n",
    "        return 'right'\n",
    "    elif(face_coords[3] > bbox[3]):\n",
    "        return 'down'\n",
    "    \n",
    "    return 'center'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74168ef0-db7e-4501-bb36-af22d0165c32",
   "metadata": {},
   "source": [
    "### Detecting the face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352cb6d-3207-4f9c-8bee-15fe6523ff7a",
   "metadata": {},
   "source": [
    "We will now go ahead and detect a face from a frame. Our function `get_prediction` takes a model and a frame and returns a dictionary with the location of all the faces found in the image and the degree of confidence that it is a face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e7f6ead-9daf-4760-84f2-c2d538d044a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(net, frame):\n",
    "    '''\n",
    "    This function takes the frame as input to the model and\n",
    "    gets the prediciton of whether it has a face or not.\n",
    "\n",
    "    It returns a dictionary with the coordinates of the face:\n",
    "    {\n",
    "      'Top': (TopX, TopY),\n",
    "      'Bottom': (BottomX, BottomY) }\n",
    "    '''\n",
    "    #Will hold the dictionary of coordinates\n",
    "    face_coordinates = []\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    # We will first create a blob from the image\n",
    "    # A blob is an image/images that have the same depth,\n",
    "    # shape (width, height) and that have been\n",
    "    # preprocessed in the same manner\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "                                 scalefactor=1.0,\n",
    "                                 size=(300,300),\n",
    "                                 mean=(104.0, 177.0, 123.0))\n",
    "    \n",
    "    # The function blobFromImage returns a 4-D tuple like so:\n",
    "    # (num_images, num_channels, width, height)\n",
    "\n",
    "    # We then feed our blob into the net\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # We perform a feed foward across\n",
    "    # all layers to get a prediction\n",
    "    predictions = net.forward()\n",
    "\n",
    "    # The forward() function also returns a 4-D tuple like so:\n",
    "    # (1, 1, 200, 7)\n",
    "    # 1, 1 - number of images working on\n",
    "    # 200 - number of faces detected\n",
    "    # 7 - a vector of 7 values like so:\n",
    "    # [Image number, Binary (0 or 1), confidence score (0 to 1),\n",
    "    # StartX, StartY, EndX, EndY]\n",
    "\n",
    "    # With this data, we can filter based on confidence score\n",
    "    # we iterate through every face\n",
    "    for face in range(predictions.shape[2]):\n",
    "        #get confidence\n",
    "        confidence = predictions[0, 0, face, 2]\n",
    "\n",
    "        if confidence > 0.5:\n",
    "            #Take the coordinates\n",
    "            (Top_x, Top_y) = predictions[0, 0, face, 3:5] * np.array([w, h])\n",
    "            (Bott_x, Bott_y) = predictions[0, 0, face, 5:] * np.array([w, h])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        face_coordinates.append({\n",
    "            'top': (int(Top_x), int(Top_y)),\n",
    "            'bottom' : (int(Bott_x), int(Bott_y)),\n",
    "            'confidence' : confidence\n",
    "        })\n",
    "    return face_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60497c-0914-4021-bcbe-e5cf313bb77d",
   "metadata": {},
   "source": [
    "The function does quite a lot, hence it is important to take some time and see that you understand what it is doing. All the steps are documented above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884ab89-e488-41fa-9d2a-f0d1c54f9d84",
   "metadata": {},
   "source": [
    "## The main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e44c7-27e1-49c4-881d-c11a965ab8ae",
   "metadata": {},
   "source": [
    "Now that we have defined all our methods, we can get into the main part of the execution. Our steps are simple:\n",
    "* We load the model\n",
    "* Create a `VideoCapture` object\n",
    "* Create a bounding box\n",
    "* Start the main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e7fec-c66d-400c-8161-cea4fa243129",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32388eef-497c-448f-bca2-76b8fec3e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the model\n",
    "net = cv2.dnn.readNetFromCaffe('model/deploy.prototxt',\\\n",
    "                               'model/res10_300x300_ssd_iter_140000.caffemodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bde40-0df8-4674-9f58-1681756f310b",
   "metadata": {},
   "source": [
    "We are using a caffe framework face detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981fccb-dcfb-46f3-99e2-6599a1f3956b",
   "metadata": {},
   "source": [
    "### VideoCapture object and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b80693-f40c-4b95-88ff-1b5a034ec2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We open the VideoCapture object here to be able\n",
    "# to create a bounding box\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "#create a bounding box: w=180, h=185\n",
    "top_x, top_y = int(frame_width//2 - 90), int(frame_height//2 - 85)\n",
    "bottom_x, bottom_y = int(frame_width//2 + 90), int(frame_height//2 + 100)\n",
    "bbox = [top_x, top_y, bottom_x, bottom_y]\n",
    "\n",
    "#assign in tuples\n",
    "centre_top = (top_x, top_y)\n",
    "centre_bottom = (bottom_x, bottom_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24297dc5-0484-4b5d-bead-21050f3b8d3f",
   "metadata": {},
   "source": [
    "We create a bounding box specifying the top left and bottom right corners of our image. We put these values inside `bbox` and `centre_top` and `centre_bottom`. We do this since the values are needed in other functions and this format allows us to pass them easily to those functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00256214-09cd-4f70-9ba5-ea0a27569595",
   "metadata": {},
   "source": [
    "### Define necessary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1612dca-cf7f-46ce-92c3-e770da150252",
   "metadata": {},
   "source": [
    "We define 3 variables:\n",
    "* `init` - to check for initialization, if it is 0, we cannot start playing. We will see what triggers it in the main loop.\n",
    "* `last_move` and `move` - these describe the last and current position of the face's bounding box w.r.t the main bounding box. We will see the necessity of recording the last move in the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4459cfe7-a72a-443e-9cb5-903d56221f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = 0\n",
    "move = ''\n",
    "last_move = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452276b-c501-4c22-affd-717ff0b24aa6",
   "metadata": {},
   "source": [
    "### The while loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c96fd42-2211-4a39-8140-9ab093e4e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the main loop\n",
    "while True:\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret is False:\n",
    "        break\n",
    "\n",
    "    # Flip frame to match normal movement\n",
    "    frame = cv2.flip(frame, 2)\n",
    "\n",
    "    # Get predictions\n",
    "    faces = get_predictions(net, frame)\n",
    "\n",
    "    # If we find a face\n",
    "    if len(faces) > 0:\n",
    "        # Sort according to highest confidence,\n",
    "        # in case of many points\n",
    "        sorted_faces = sorted(faces, key=lambda x: x['confidence'],\n",
    "                              reverse=True)\n",
    "\n",
    "        # Take the best sorted image\n",
    "        face = sorted_faces[0]\n",
    "\n",
    "        #Get the face coordinates\n",
    "        face_top_x, face_top_y = face['top']\n",
    "        face_bott_x, face_bott_y = face['bottom']\n",
    "        \n",
    "        #draw face bounding rectangle on the frame\n",
    "        draw_rectangle((face_top_x, face_top_y),\n",
    "                        (face_bott_x, face_bott_y),\n",
    "                        frame, color=(0, 0, 255))\n",
    "    \n",
    "        # check whether the face is in bbox:\n",
    "        face_coords = [face_top_x, face_top_y, face_bott_x, face_bott_y]\n",
    "\n",
    "        move = face_inbox(bbox, face_coords)\n",
    "\n",
    "        cv2.putText(frame, move, (30, 50), 0, 3, (180, 0, 180), 2)\n",
    "\n",
    "        #initialize game\n",
    "        if init == 0:\n",
    "            if move == 'center':\n",
    "                init = 1\n",
    "                cv2.putText(frame, 'Initialized', (30, 50), 0, 3, (10, 100, 180), 10)\n",
    "        else:\n",
    "            if last_move == 'center':\n",
    "                move_key(move)\n",
    "\n",
    "        last_move = move\n",
    "    \n",
    "    \n",
    "    # Draw the central bounding box\n",
    "    draw_rectangle(centre_top, centre_bottom, frame)\n",
    "\n",
    "    cv2.imshow('current frame', frame)\n",
    "\n",
    "    key = cv2.waitKey(5)\n",
    "    if key == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853216e-85bb-40d0-b117-88038bbb3ede",
   "metadata": {},
   "source": [
    "Let me explain what is happening in the loop above:\n",
    "* We first ensure that the video is opened\n",
    "* We then try to read a frame using cap.read(), if the frame cannot be read, we break loop\n",
    "* We flip the frame, horizontally so that it will better match our movement.\n",
    "* We use the `get_predictions` method to get the coordinates of a detected face.\n",
    "* If we found no face, we skip the whole section and only draw the outer bounding box, else\n",
    "* If we did:\n",
    "* we sort the fac.es based on confidence\n",
    "* take the first face\n",
    "* get the face coordinates\n",
    "* draw the face bounding rectangle in the frame\n",
    "* detect the face movement using `face_inbox`\n",
    "* write on screen the position of the face w.r.t the bounding box\n",
    "* check if the game is initialized. The game is initialized when the face moves to the centre of the bounding box and `init` is set to 1.\n",
    "* If the game is initialized, we will only press a key if the last move was 'centre' to reduce the number of accidental moves and lag.\n",
    "* Lastly we show the frame, and exit the loop if the user presses `Esc` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dc440-6dd3-4b9a-9f9e-881174203d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b8dee6-5dd7-452b-98dd-06889d116909",
   "metadata": {},
   "source": [
    "We then release the VideoCaoture object and destroy all windows after the while loop. And there, we have it. With this code,we can be able to play video games only using head movements. We can speed it up by capturing every other frame. You can get more from this by running on an IDE than on the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
