{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56136b89-a4e5-4588-804d-a25668c5d953",
   "metadata": {},
   "source": [
    "# Using Homography for Image Registration and Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e45d2b-0cb9-4196-9ebe-8faaf34f3abb",
   "metadata": {},
   "source": [
    "We will use homography for image registration. For this, we will take an image template and align a scanned form according to the template. This will require:\n",
    "* Finding keypoints and features in each image\n",
    "* Matching the features and keypoints from each image\n",
    "* Sorting for best matches\n",
    "* Finding homography between best matches\n",
    "* Aligning the scanned image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126722c-ab62-440c-bda4-9bb223e9d6df",
   "metadata": {},
   "source": [
    "### Loading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384939d-21fa-4944-96bb-e719e10b7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677785bd-f5d5-4f13-a266-5385ac7833cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = cv2.imread('images/form.jpg')\n",
    "scanned = cv2.imread('images/scanned-form.jpg')\n",
    "\n",
    "# Display the images. \n",
    "plt.figure(figsize = [20, 10])\n",
    "plt.subplot(121); plt.axis('off'); plt.imshow(template[:, :, ::-1]); plt.title(\"Original Form\")\n",
    "plt.subplot(122); plt.axis('off'); plt.imshow(scanned[:, :, ::-1]); plt.title(\"Scanned Form\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80837f6d-85d3-44a9-a141-9623ab6f3978",
   "metadata": {},
   "source": [
    "Now that we have the forms, the next step is to identify the keypoints in each image, things that are unique or stand out in each image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd4e98-f924-4fcd-bdee-cb6e8a4020bb",
   "metadata": {},
   "source": [
    "### Keypoint Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba47b9e-64a9-4a07-82ff-eb9858564f6e",
   "metadata": {},
   "source": [
    "OpenCV provides many classes and algorithms for finding features. Features are elements that describe the image such as corners e.t.c. Some of them include: FAST, BRIEF, SIFT SURF, ORB and Corner detectors.\n",
    "\n",
    "Feature detection is the task of finding these features. Once we find them, we also need to be able to describe them so that we can search for them in other images. Most of the classes described above inherit from the virtual class `features2D`. To find features, we can use the `detect` method to detect features and `compute` to describe them. Another weay to do this is to use the `detectandCompute` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144261f0-8723-4a36-8233-7078386bdeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use ORB\n",
    "#creating an ORB object\n",
    "orb_object = cv2.ORB_create(nfeatures=600)\n",
    "\n",
    "keypoints1, descriptors1 = orb_object.detectAndCompute(template, None)\n",
    "keypoints2, descriptors2 = orb_object.detectAndCompute(scanned, None)\n",
    "\n",
    "#We can drawn the keypoints\n",
    "template_keypoints = cv2.drawKeypoints(template, keypoints1, None, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "scanned_keypoints = cv2.drawKeypoints(scanned, keypoints2, None, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the keypoint images. \n",
    "plt.figure(figsize = [20, 10])\n",
    "plt.subplot(121); plt.axis('off'); plt.imshow(template_keypoints[:, :, ::-1]); plt.title(\"Original Form KP\")\n",
    "plt.subplot(122); plt.axis('off'); plt.imshow(scanned_keypoints[:, :, ::-1]); plt.title(\"Scanned Form KP\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e47209-6873-4f4f-8a0e-e18bc3c9a0bb",
   "metadata": {},
   "source": [
    "### Examining Keypoints and Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58593908-8cb4-4465-963d-b884a041e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(keypoints1))\n",
    "print(type(keypoints1[0]))\n",
    "print(len(keypoints1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de498db-e0e4-4c97-a089-87e4764c2aff",
   "metadata": {},
   "source": [
    "We see that the keypoints are a tuple of keypoints of the size `nfeatures`. Each keypoint is an object of the class `cv.Keypoint` whose atrributes include angle, pt (x, y coordinate of position in image) and size (of neighborhood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434199e5-ef48-4a82-b620-450c3e13d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Orientation', keypoints1[0].angle)\n",
    "print('X = ', keypoints1[0].pt[0], ' Y = ', keypoints1[0].pt[1])\n",
    "print('Size = ', keypoints1[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f97b1-b143-4796-a476-5a86afe25582",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(descriptors1))\n",
    "print(descriptors1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c673a2-d84c-4bf9-8468-1c8c45eab8ed",
   "metadata": {},
   "source": [
    "The descriptors a simply an array of arrays of the same length as the descriptors and 32 descriptions in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e035eb2-3e18-48d2-a77d-7b85cc298c1d",
   "metadata": {},
   "source": [
    "Now that we have the keypoints and descriptors for each image, we can go on and match them up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c1f67-8a75-42bb-b647-1ce67e0d49fb",
   "metadata": {},
   "source": [
    "### Feature Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e575676-5c2b-491e-8518-82fc09fc3dca",
   "metadata": {},
   "source": [
    "We will use the brute-force matcher. More details [here](https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html). We can call the method directly from the class `BFMatcher` or use the parent class `DescriptorMatcher` which creates either a Flann-based of BF matcher object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85c53e-f11c-4d96-b788-60598bcf7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the object\n",
    "matcher = cv2.BFMatcher.create(cv2.NORM_HAMMING, True)\n",
    "\n",
    "#using our matcher object to match\n",
    "matches_normal = matcher.match(descriptors1, descriptors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c6cd7-4543-413e-a3d9-eb2d76f531dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(matches_normal[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ced1c3-8af0-401a-9876-27f53ec98303",
   "metadata": {},
   "source": [
    "The `match` method returns a tuple of `DMatch` objects each of which contain: (distance) the distance between descriptors, imgIdx (train image index), queryIdx (query descriptor index) and trainIdx (train descriptor index). We can sort the matches according to distance and only keep a portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2e09e-1032-4883-8dcb-9b75c4f061c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_normal = sorted(matches_normal, key=lambda x:x.distance, reverse=False)\n",
    "matches_normal = matches_normal[:int(len(matches_normal) * .1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a92e0-114d-491b-9a9b-e862104a6305",
   "metadata": {},
   "source": [
    "Drawing the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006299a3-03a7-40db-a61a-bf20ebd600da",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawn_matches = cv2.drawMatches(\n",
    "            template,\n",
    "            keypoints1,\n",
    "            scanned,\n",
    "            keypoints2,\n",
    "            matches_normal,\n",
    "            None,\n",
    "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.imshow(drawn_matches[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d55864-a46e-4464-8341-b1ee9e094180",
   "metadata": {},
   "source": [
    "### Using Homography to align scanned image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa59530-aee4-49b6-aa3f-c53cb39961fc",
   "metadata": {},
   "source": [
    "We compute the homographies of the best descriptors of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e7b41-1a3e-49c6-9250-c2a9e571c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the location of good matches.\n",
    "points1 = np.zeros((len(matches_normal), 2), dtype = np.float32)\n",
    "points2 = np.zeros((len(matches_normal), 2), dtype = np.float32)\n",
    "\n",
    "for i, match in enumerate(matches_normal):\n",
    "    points1[i] = keypoints1[match.queryIdx].pt\n",
    "    points2[i] = keypoints2[match.trainIdx].pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f932666-fcce-4c19-9949-d6325f49bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, mask = cv2.findHomography(points2, points1, cv2.RANSAC)\n",
    "height, width = template.shape[:2]\n",
    "scanned_restored = cv2.warpPerspective(scanned, h, (width, height))\n",
    "\n",
    "plt.imshow(scanned_restored[:, :, ::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
