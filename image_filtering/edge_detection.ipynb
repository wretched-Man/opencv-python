{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6d7f2a-9449-4c58-b6a7-11537458e136",
   "metadata": {},
   "source": [
    "# Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabee75-eb32-4cfe-a3ae-b20f00df0a6b",
   "metadata": {},
   "source": [
    "We will now look at the problem of edge detection. We can define an edge as a rapid change in image intensity within a small region. We will look at edge detection with OpenCV. We will examine 3 functions:\n",
    "* Sobel edge detection\n",
    "* Laplaican edge detection\n",
    "* Canny Edge detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8c868-9d25-48b0-ba61-53957cc3b0d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sobel Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912542f5-015a-44c8-bd1e-188f687c1bd3",
   "metadata": {},
   "source": [
    "The Sobel Edge detection algorithm takes advantage of the fact that an edge is a rapid change in intensity. We know from calculus that the first derivative of a function gives us the instantenious change of a function at that point. Hence, the Sobel operator works by getting the first derivative of the image - or more exactly yet - the gradient (a pair of values), is found as the partial derivative w.r.t `x` and the partial derivitive w.r.t `y`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05e54122-6efc-4cfa-be2f-5bb12b07ae61",
   "metadata": {},
   "source": [
    "These two values of the gradient can also be represented as a convolution with two kernels, the Sobel X kernel to approximate the X part of the gradient and the Y kernel to approximate the Y part.\n",
    "<br><br>\n",
    "* X-direction Kernel : $$\\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "* Y-direction Kernel : $$\\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix}$$\n",
    "\n",
    "The result of convolving an image with both kernels is the 'Sobel Edge Map'.\n",
    "\n",
    "Let us try with an example image. We will convolve with the X-direction kernel to get the vertical edges and with the Y-direction kernel to get the horizontal edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630644ae-6994-4976-805c-354bcd24229a",
   "metadata": {},
   "source": [
    "### Convolving with the X-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37be0cf-1837-433d-a7b3-4dc5bdbeeee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192deaeb-6570-4ac8-8a84-cefa78e58102",
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = cv2.imread('images/checkerboard_color.png', cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95d522-2bda-479e-95eb-200abf90656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "plt.imshow(checker, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec4a79-11f8-4fa1-978d-b4dfd31b2a13",
   "metadata": {},
   "source": [
    "We want to detect edges in the X-direction using the X-kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d6d95-ec3c-4c30-b3b7-7acc86520b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xkernel = np.array([[-1, 0, 1],\n",
    "                   [-2, 0, 2],\n",
    "                   [-1, 0, 1]])\n",
    "sobel_xfiltered = cv2.filter2D(checker, cv2.CV_64F, xkernel)\n",
    "plt.imshow(sobel_xfiltered, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1ae74-9b72-49cc-8734-006acb1185f6",
   "metadata": {},
   "source": [
    "### Convolving with the Y-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee187fb2-8a36-497b-bfe5-8013c798010c",
   "metadata": {},
   "source": [
    "We will now use the Y-kernel to detect horizontal edges. As with the X-kernel above, you notice that we use `cv2.CV_64F` for depth so as to capture all the ranges of values produced in the convolution operation. A depth allowing more values than 256 allows us more options in choosing the threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1eb4ec-f22c-46a0-9746-821bca07c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the y-kernel is similar to the x-kernel rotated by 90\n",
    "ykernel = np.rot90(xkernel)\n",
    "sobel_yfiltered = cv2.filter2D(checker, cv2.CV_64F, ykernel)\n",
    "plt.imshow(sobel_yfiltered, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be68ae-f9ee-483c-86bf-2ef9aab7e336",
   "metadata": {},
   "source": [
    "Getting the complete Sobel edge map is similar to getting the magnitude of each edge. We can get an approximate magnitude by simply adding the X and Y maps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab764c-c012-4146-b9ea-4ef54092bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_filtered = sobel_xfiltered + sobel_yfiltered\n",
    "plt.imshow(sobel_filtered, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7110452-2ae2-4216-b5a7-de331b93780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sobel_thresh = cv2.threshold(sobel_filtered, -1, np.max(sobel_filtered), cv2.THRESH_BINARY_INV)\n",
    "plt.imshow(sobel_thresh, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af591f40-e3d1-4d0f-9445-8f88850bef25",
   "metadata": {},
   "source": [
    "We can create the same filter using OpenCV's `Sobel` method. We select the X or Y derivitive by setting the value of `dx` and `dy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad59f13-8c66-4c39-9c8e-fad10fe5e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelx = cv2.Sobel(checker, cv2.CV_64F, 1, 0, ksize=3) #dx, dy (1, 0)\n",
    "sobely = cv2.Sobel(checker, cv2.CV_64F, 0, 1, ksize=3) #dx, dy (0, 1)\n",
    "plt.figure(figsize=[15, 3])\n",
    "plt.subplot(121); plt.imshow(sobelx, cmap='gray'); plt.title('X - kernel')\n",
    "plt.subplot(122); plt.imshow(sobely, cmap='gray'); plt.title('Y - kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e838211-e85e-4e71-bfa7-520ad772b1b0",
   "metadata": {},
   "source": [
    "Getting the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3b868-5615-4d06-87cc-870c00d4b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_full = np.sqrt(sobelx**2 + sobely**2)\n",
    "plt.imshow(sobel_full, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71eff0-49e8-41e6-9f0e-4922adbbd9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sobel_thresh = cv2.threshold(sobel_full, 1, np.max(sobel_full), cv2.THRESH_BINARY)\n",
    "plt.imshow(sobel_thresh, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04dd78e-28b1-43e9-ac13-56fd91e81ba1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Is the Sobel Kernel correlated or convoluted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e58c2-be67-4175-8a39-586df7a57e72",
   "metadata": {},
   "source": [
    "Convolution differs from correlation in that in correlation, the kernel is flipped twice, (along the Y-axis then the X-axis). This is no problem if the kernel is symmetrical along the diagonal. However, it becomes a problem if it is not. Let us try it out with the Sobel kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d44ad6-fba6-49be-8786-4ce40a883cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpadded = np.pad(xkernel, (398, 399))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701cb9d9-ffe8-41a5-84b9-b09b3052f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpadded_fft_shift = np.fft.fftshift(np.fft.fft2(xpadded))\n",
    "checker_fft_shift = np.fft.fftshift(np.fft.fft2(checker))\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(121); plt.imshow(np.abs(xpadded_fft_shift), cmap='gray'); #plt.title('X - kernel')\n",
    "plt.subplot(122); plt.imshow(np.abs(np.log(checker_fft_shift) + 1), cmap='gray'); #plt.title('Y - kernel #np,log is used for visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850cd112-d4dd-4910-b77f-2b808162a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_edge_fft = xpadded_fft_shift * checker_fft_shift\n",
    "plt.figure(figsize=[4, 4])\n",
    "plt.imshow(np.abs(np.log(sobel_edge_fft + (1/np.abs(np.max(sobel_edge_fft))**51))), cmap='gray') #to remove log(0) but also avoid much interference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24931ec2-f9b0-4669-9a43-b5582337b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_edge_ifft = np.fft.ifft2(np.fft.ifftshift(sobel_edge_fft))\n",
    "plt.imshow(sobel_edge_ifft.real, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69866d99-d7c4-40c6-9cb1-91542ae09027",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sobel_edge_ifft.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f55c0-2395-4122-b377-8986dbec5324",
   "metadata": {},
   "source": [
    "### Convolving both kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40182c-6ef2-4305-adab-04bb4c67810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypadded = np.pad(ykernel, (398, 399))\n",
    "ypadded_fft_shift = np.fft.fftshift(np.fft.fft2(ypadded))\n",
    "one_sobel_kernel = ypadded_fft_shift * xpadded_fft_shift\n",
    "plt.imshow(np.abs(one_sobel_kernel), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b685e-5070-4e91-9f0b-63a66a047db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sobel_map = one_sobel_kernel * checker_fft_shift\n",
    "final_sobel = np.fft.ifft2(np.fft.ifftshift(one_sobel_map))\n",
    "\n",
    "#standardize\n",
    "#final_sobel = final_sobel + np.abs(np.min(final_sobel))\n",
    "plt.figure(figsize=[10, 5])\n",
    "plt.subplot(121); plt.imshow(np.abs(np.log(one_sobel_map + (1/np.abs(np.max(one_sobel_map))**51))), cmap='gray');\n",
    "plt.subplot(122); plt.imshow(np.abs(final_sobel), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd2fb4-9078-412b-822d-57d39a608361",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sobel.real[199, 499]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd55fc-70af-4527-a960-516b55c1400e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Laplacian Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e87dce-cfb7-49df-84a0-6774cc8e23bf",
   "metadata": {},
   "source": [
    "We have seen that the Sobel operator can be used to find edges by detecting rapid changes in intensity either in the X or Y direction. We have also seen that the operator produces two values: for X and Y and gives us the magnitude and location of an edge. The local maxima is what enables us to know whether a point defines an edge or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cba500-8ed6-4661-b14f-7782c91ccf53",
   "metadata": {},
   "source": [
    "There  is also another operator known as the Laplacian Operator. The Laplacian Operator uses the second derivitive to detect edges. At the point of the local maxima in the first derivative, the is no change making the value 0 at the second derivative, but then there is a rapid decline of values immediately after. Hence, the edge is identified at the point of zero-crossing where the value quickly changes to negative as it crosses the local maxima in the first derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e4636-6b33-4b8a-9b1e-401e1f715b7f",
   "metadata": {},
   "source": [
    "Unlike the Sobel operator, the Laplacian operator produces one value since it is found by adding the second derivative w.r.t the X-direction and the second derivative w.r.t the Y-direction, hence resulting in one value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2927f29a-eb6c-4add-95bf-4cfa8571948f",
   "metadata": {},
   "source": [
    "### An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ff90f-87a2-4d06-b07b-ab1083c7d874",
   "metadata": {},
   "source": [
    "We can use OpenCV's `Laplacian` method to find the Laplacian edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59b03f-e926-4dcc-ab07-0878bd043641",
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = cv2.imread('images/red_hibiscus.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(icon, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc094a-d596-48c1-b893-ea261d163928",
   "metadata": {},
   "outputs": [],
   "source": [
    "icon_laplace = cv2.Laplacian(icon, cv2.CV_64F, ksize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e2edc-a336-4200-98ab-7c900a74e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(icon_laplace, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaab851-d7a7-4f25-82a4-258d27326a3f",
   "metadata": {},
   "source": [
    "Although difficult to see, the result of the Laplacian clearly marks out the flower's edges. To see it better, we can choose a ROI. From it we can clearly see the flower image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0abe738-450c-4880-a48b-a03db713a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(icon_laplace[420:1000, 200:850], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd1aad-76c0-41c7-8737-1419ae52e185",
   "metadata": {},
   "source": [
    "Next, we threshold the image to separate edges from non-edges. The threshold value represents the zero-crossings from the second derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7193af27-cfb1-47ef-8d1a-a71c52d5581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, icon_laplace_thresh = cv2.threshold(icon_laplace, 35, np.max(icon_laplace), cv2.THRESH_BINARY)\n",
    "plt.imshow(icon_laplace_thresh, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8820ef1-0fce-4307-9910-7c0d2ecbabdd",
   "metadata": {},
   "source": [
    "We have seen two edge-detection algorithms that rely on gradients to find the edges. The advantage of the Sobel operator is that it gives us both the magnitude of an edge and the direction. However, it requires two convolution operations. On the other hand, the Laplacian method requires only one convolution operation although it only tells us that there exists an edge at a certain point, not the magnitude or the direction of the edge. There is a method that combines both the above, taking the advantages of both, which we will look at in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a36b46-c106-4077-87ff-d5f8cd75bb70",
   "metadata": {},
   "source": [
    "## Reducing Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0081223-87b8-4377-addd-feff4f15d0b1",
   "metadata": {},
   "source": [
    "Noise affects the number and quality of edges we detect. Noise in an image can amplify non-edges to be recognized as edges. We can reduce noise by using Gaussian smoothing. As you know, a Gaussian is a smoothing linear operator that attenuetes the higher frequencies and leaves the low-frequency components in an image. Applying the edge detection algorithm on the Gaussian smoothed image allows us to get the high-level edges such as outlines and not the detailed edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdf3ea-b8d4-4202-a00b-3f7d36258976",
   "metadata": {},
   "source": [
    "The steps for edge detection with smoothing involve:\n",
    "1. Convolving the Gaussian with the image to smooth the image\n",
    "2. Applying the Sobel/Laplacian operator on the smoothed image to detect edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36380de4-e63e-41c9-b3e4-7a966d31b9f6",
   "metadata": {},
   "source": [
    "We can reduce the number of computations in the above steps with both the gradient operator(Sobel) and the Laplacian operators by understanding that the Gaussian operator and the gradient operators are linear. Hence, associatively, we can:\n",
    "1. Apply the Sobel/Laplacian operator to the Gaussian.\n",
    "2. Convolve the image with the new operator from 1 above; the **derivative of Gaussian** for Sobel and the **Laplacian of Gaussian** for Laplacian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8b942-3ea7-481a-97c9-f055b9b3e297",
   "metadata": {},
   "source": [
    "### An Example (Laplacian of Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf91d8-6938-4662-aabb-6d6957df05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to detect high-level edges from\n",
    "# the flower above using the LoG operator.\n",
    "#the gaussian low pass filter\n",
    "import math\n",
    "def distance(point1,point2):\n",
    "    return math.sqrt((point1[0]-point2[0])**2 + (point1[1]-point2[1])**2)\n",
    "\n",
    "def gaussianLP(D0,imgShape):\n",
    "    base = np.zeros(imgShape[:2])\n",
    "    rows, cols = imgShape[:2]\n",
    "    center = (rows/2,cols/2)\n",
    "    for x in range(cols):\n",
    "        for y in range(rows):\n",
    "            base[y,x] = math.exp(((-distance((y,x),center)**2)/(2*(D0**2))))\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd93a6-d29f-4f8f-9a5e-0511926e0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Gaussian Smoothing\n",
    "def convolve(image, filter_used):\n",
    "    img_in_freq_domain = np.fft.fft2(image)\n",
    "\n",
    "    # Shift the zero-frequency component to the center of the frequency spectrum\n",
    "    centered = np.fft.fftshift(img_in_freq_domain)\n",
    "\n",
    "    # Multiply the filter with the centered spectrum\n",
    "    filtered_image_in_freq_domain = centered * np.fft.fftshift(np.fft.fft2(filter_used))\n",
    "\n",
    "    # Shift the zero-frequency component back to the top-left corner of the frequency spectrum\n",
    "    inverse_fftshift_on_filtered_image = np.fft.ifft2(filtered_image_in_freq_domain)\n",
    "\n",
    "    # Apply the inverse Fourier transform to obtain the final filtered image\n",
    "    final_filtered_image = np.fft.ifftshift(inverse_fftshift_on_filtered_image)\n",
    "\n",
    "    return np.abs(final_filtered_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bce6e4-4365-4323-aed6-6a1208cf107a",
   "metadata": {},
   "source": [
    "Above code was sourced from: [Source](https://medium.com/@mahmed31098/image-processing-with-python-frequency-domain-filtering-for-noise-reduction-and-image-enhancement-d917e449db68 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a550abd-d8ab-4b7d-ad29-0cbbc2511c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the Gaussian\n",
    "sigma=10\n",
    "gauss_flower = gaussianLP(sigma, icon.shape)\n",
    "#We will now create a LoG for the Gaussian of the image\n",
    "log_flower = cv2.Laplacian(gauss_flower, cv2.CV_64F, ksize=3)\n",
    "\n",
    "plt.figure(figsize=[8, 8])\n",
    "plt.subplot(121); plt.imshow(gauss_flower, cmap='gray'); plt.title(f'Gaussian, Sigma = {sigma}')\n",
    "plt.subplot(122); plt.imshow(log_flower, cmap='gray'); plt.title('Laplacian of Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80fe96-30e7-4afe-9aae-66e4ae05919b",
   "metadata": {},
   "source": [
    "Now we have the Laplacian of Gaussian for the image. We can then convolve it with the image. It is important to note that the parameter `D0` controls the width of the Gaussian. Increasing the diameter increases the number of frequencies that pass, hence the number of edges is increased. We will see the effect of this change in a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97864d80-942f-4a88-95db-181709c7949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now convolve the LoG with the image\n",
    "# The `filter2D` correlates but since the gaussian is\n",
    "# reflected along the diagonal, this should be no\n",
    "# problem.\n",
    "\n",
    "# Getting the edge map\n",
    "log_flower_edges = convolve(icon, log_flower)\n",
    "\n",
    "#Thresholding to choose the zero-crossing for the edges\n",
    "_, log_flower_zeros = cv2.threshold(log_flower_edges, 280, np.max(log_flower_edges), cv2.THRESH_BINARY)\n",
    "\n",
    "#displaying\n",
    "plt.figure(figsize=[7, 8])\n",
    "plt.subplot(121); plt.imshow(log_flower_edges, cmap='gray'); plt.title('Edge Map')\n",
    "plt.subplot(122); plt.imshow(log_flower_zeros, cmap='gray'); plt.title('Edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7b75e-67e1-4e4f-82b9-d74cca0c8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(log_flower_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0c927-df4d-48ad-948c-72ffb4ab2fa1",
   "metadata": {},
   "source": [
    "Here, we can see that we only captured the high level and outer edges of the flower as opposed to the first image where we captured even the interior details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44425c7a-e8ca-43f5-a6f9-0f8f2277fed1",
   "metadata": {},
   "source": [
    "### Effect of Sigma on number of edges captured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f48d9-b1f2-49aa-8673-c2e34edf5e11",
   "metadata": {},
   "source": [
    "We will briefly see the effect of Sigma on the number of image edges found. We will vary the threshold value for each image to find the zero-crossing that works best for each edge map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddd48d-dfcc-4187-906e-f5ef6690a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that creates a LoG operator, convolves with\n",
    "# image and produces an output of the threshold image\n",
    "def logedges(sigma, threshold_value):\n",
    "    lpgaussian = gaussianLP(sigma, icon.shape[:2]) #low-pass gaussian\n",
    "    logoperator = cv2.Laplacian(lpgaussian, cv2.CV_64F, ksize=3) #LoG operator\n",
    "    \n",
    "    #convolving\n",
    "    edgemap = cv2.filter2D(icon, cv2.CV_64F, logoperator)\n",
    "    #thresholding\n",
    "    _, loc_edges = cv2.threshold(edgemap, threshold_value, np.max(edgemap), cv2.THRESH_BINARY)\n",
    "\n",
    "    return edgemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510cedb-2f4a-4f48-b167-3f021f3f497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_1 = logedges(1, 35)\n",
    "edges_5 = logedges(5, 70)\n",
    "edges_10 = logedges(10, 135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd790e-962e-4a5e-81ae-40aba5e886d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.subplot(131); plt.imshow(edges_1, cmap='gray'); plt.title('Sigma 1')\n",
    "plt.subplot(132); plt.imshow(edges_5, cmap='gray'); plt.title('Sigma 5')\n",
    "plt.subplot(133); plt.imshow(edges_10, cmap='gray'); plt.title('Sigma 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b498e31-e677-4cbb-9ae5-03a023bc5baf",
   "metadata": {},
   "source": [
    "We can see that as the sigma level increases we can only capture high-level edges. We see that using sigma = 1 captures as edges much of the flower, whereas sigma = 5 captures less of the interior details but more of the exterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00407476-6cdc-450a-9a94-10dd592137e8",
   "metadata": {},
   "source": [
    "## Canny Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36583933-77f3-4e1d-90bb-b59df8038e5e",
   "metadata": {},
   "source": [
    "The canny edge detection algorithm is one of the most widely used edge-detection algorithms. It uses aspects of the Sobel and Laplacian operators.\n",
    "\n",
    "It follows the following steps:\n",
    "1. Gaussian smoothing to remove noise.\n",
    "2. Find the intensity gradient at each pixel.\n",
    "3. Non-maximum suppression - for every pixel, check whether it is the maximum in its local neighborhood, suppress if not.\n",
    "4. Double thresholding - to segment strong, weak and no edges.\n",
    "5. Hysteresis thresholding to determine if the weak edges are connected to strong edges and and hence to count or, if not, make them irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0f880-6a51-4c3e-b0c3-9fe74302d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the image\n",
    "face = cv2.imread('images/face_2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(face, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875415dd-08da-4b38-a32f-5aa8ff4963fe",
   "metadata": {},
   "source": [
    "### Getting the intensity gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d659ecb5-e3a3-4c30-acdb-e8a183df1623",
   "metadata": {},
   "source": [
    "We are going to begin by getting the image intensity gradient to calculate the magnitude and orientation of the image. The method `derivative_gaussian` will return the magnitude and the gradient orientation of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ccf7d-2e2d-40e4-bf39-7f1337ca5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the derivative of Gaussian\n",
    "# the gradient magnitude\n",
    "# the gradient orientation\n",
    "\n",
    "def derivative_gaussian(img, sigma=1):\n",
    "    #smoothing\n",
    "    face_gaussian = convolve(img, gaussianLP(sigma, img.shape))\n",
    "    \n",
    "    #getting the x and y Sobel gradients\n",
    "    Ix = cv2.Sobel(face_gaussian, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    Iy = cv2.Sobel(face_gaussian, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "    magnitude = np.hypot(Ix, Iy)\n",
    "    magnitude = magnitude/magnitude.max() * 255\n",
    "    direction = np.arctan2(Ix, Iy)\n",
    "\n",
    "    return magnitude, direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebb4b7-92f0-4b56-b3f8-d4831d4a8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgemap_other, direction_map = derivative_gaussian(face, 2.2)\n",
    "plt.imshow(edgemap_other, cmap='gray')\n",
    "_ = plt.title('Gradient Magnitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91eba4f-14cb-404f-89b3-f46145651f61",
   "metadata": {},
   "source": [
    "### Non-maximum suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a73204-882f-4abf-98ef-194a0ddf20be",
   "metadata": {},
   "source": [
    "Ideally, the edges must be thin. Hence we will apply non-maximum suppression to the gradient magnitude to suppress values that are not maximum in their local neighborhood.\n",
    "\n",
    "Sources: [1](https://towardsdatascience.com/canny-edge-detection-step-by-step-in-python-computer-vision-b49c3a2d8123), [2](https://en.wikipedia.org/wiki/Canny_edge_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247eccf-a415-4d6f-a3ef-520f2a8e3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will go through the image, at every point of grad.\n",
    "# we will find the gradient\n",
    "\n",
    "def edge_thinning_one(edgemap, orientmap):\n",
    "    final = np.zeros(edgemap.shape, np.int32)\n",
    "    for x in range(1, edgemap.shape[0] - 1):\n",
    "        for y in range(1, edgemap.shape[1] - 1):\n",
    "            p = 255\n",
    "            q = 255\n",
    "            #Get the angle\n",
    "            theta = np.abs(np.rad2deg(orientmap[x, y]))\n",
    "            if (0 <= theta < 22.5) or (157.5 <= theta <= 180):\n",
    "                # 0, 180\n",
    "                p = edgemap[x, y-1]\n",
    "                q = edgemap[x, y+1]\n",
    "            elif (22.5 <= theta < 67.5):\n",
    "                # 45\n",
    "                p = edgemap[x+1, y-1]\n",
    "                q = edgemap[x-1, y+1]\n",
    "            elif (67.5 <= theta < 112.5):\n",
    "                # 90\n",
    "                p = edgemap[x-1, y]\n",
    "                q = edgemap[x+1, y]\n",
    "            elif (112.5 <= theta < 157.5):\n",
    "                # 135\n",
    "                p = edgemap[x-1, y-1]\n",
    "                q = edgemap[x+1, y+1]\n",
    "            else:\n",
    "                # For errors\n",
    "                pass\n",
    "\n",
    "            #check if the magnitude at point is greater than p,q\n",
    "            if (edgemap[x, y] >= p) and (edgemap[x, y] >= q):\n",
    "                final[x, y] = edgemap[x, y]\n",
    "            else:\n",
    "                final[x, y] = 0\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0dc2e-4625-461d-b56c-684145b59f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thin_edges = edge_thinning_one(edgemap_other, direction_map)\n",
    "plt.figure(figsize=[7, 7])\n",
    "plt.imshow(thin_edges, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06586dcd-dc9b-4c4a-b0d1-ed2618f42296",
   "metadata": {},
   "source": [
    "### Double thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33425e6-d960-4e4b-b539-4ea6bdcde342",
   "metadata": {},
   "source": [
    "We will now apply a max and min threshold to further segment our image. Values above the maximum threshold will be considered strong edges while values below will be considered non-relevant. Values between will be considered weak edges and will only be elevated in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686b21f-9849-4673-801b-a74448e46f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_threshold(edgearray, lowbound = 40, highbound = 80):\n",
    "    final_edge_array = np.zeros(edgearray.shape, np.uint8)\n",
    "    \n",
    "    #finding the edges\n",
    "    #strong edges\n",
    "    strong_edges_x, strong_edges_y = np.where(edgearray >= highbound)\n",
    "    final_edge_array[strong_edges_x, strong_edges_y] = 255\n",
    "\n",
    "    #weak edges\n",
    "    weak_edges_x, weak_edges_y = np.where((edgearray >= lowbound) & (edgearray < highbound))\n",
    "    final_edge_array[weak_edges_x, weak_edges_y] = highbound - ((highbound-lowbound)//2)\n",
    "    \n",
    "    return final_edge_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc72ff-54ae-40fb-966d-a1be07ce7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_image = double_threshold(thin_edges, 35, 50)\n",
    "plt.figure(figsize=[10, 10])\n",
    "plt.imshow(threshold_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad32155-d77a-4a89-ae1a-573faf9aa9b5",
   "metadata": {},
   "source": [
    "### Edge tracking by hysteresis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96239649-3703-490b-969c-3c6f6e3f06dd",
   "metadata": {},
   "source": [
    "We iterate through the weak edges to see if there are any strong edges around their 8-pixel neighborhood. IF there are, we call it a strong edge, otherwise it will not make the cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a3484-f65e-40de-9d90-ae0a32c3a40c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hysteresis_thresh(edge_image):\n",
    "    final_image = edge_image.copy()\n",
    "\n",
    "    #local neighborhood\n",
    "    row = np.array([-1, -1, -1, 0, 0, 0, 1, 1, 1])\n",
    "    col = np.array([-1, 0, 1, -1, 0, 1, -1, 0, 1])\n",
    "    \n",
    "    for x in range(1, edge_image.shape[0] - 1):\n",
    "        for y in range(1, edge_image.shape[1] - 1):\n",
    "            #only the weak edges\n",
    "            #if (edge_image[x, y] > 0) and (edge_image[x, y] < 255):\n",
    "            max = 0\n",
    "            for pos in range(len(row)):\n",
    "                a = x + row[pos]\n",
    "                b = y + col[pos]\n",
    "                \n",
    "                if edge_image[a, b] == 255:\n",
    "                    max = edge_image[a, b]\n",
    "        \n",
    "            final_image[x, y] = max\n",
    "\n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b0eed-7a9a-461f-b990-029c344e45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_piece = hysteresis_thresh(threshold_image)\n",
    "plt.figure(figsize=[10, 10])\n",
    "plt.imshow(final_piece, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7d306-2238-4fa8-9b2f-0efe40a04f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(final_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760384e-a270-4bbd-b5f5-19f4765d09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(threshold_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7120ab-9121-45e5-a210-2fe3c77ba0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(thin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6b77d-3900-4e21-8371-0bede9bf8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.Canny(face, 100, 200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
